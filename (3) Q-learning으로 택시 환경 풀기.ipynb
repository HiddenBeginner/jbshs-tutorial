{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 택시 환경 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**택시 환경 공간**\n",
    "- 5 x 5 행렬 위에 Red, Green, Yellow, Blue로 표기된 4가지 장소가 고정된 위치에 배치되어 있습니다.\n",
    "\n",
    "~~~\n",
    "+---------+\n",
    "|R: | : :G|\n",
    "| : | : : |\n",
    "| : : : : |\n",
    "| | : | : |\n",
    "|Y| : |B: |\n",
    "+---------+\n",
    "~~~\n",
    "\n",
    "<br>\n",
    "\n",
    "**환경 초기화**\n",
    "- 환경 시작시, 택시는 25개 위치 중에서 랜덤하게 배치됩니다. \n",
    "- 환경 시작시, 승객은 4가지 장소 또는 택시 안에 배치됩니다.\n",
    "- 환경 시작시, 승객의 목적지가 4가지 장소 중에 한 곳으로 정해집니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "**상태 공간**\n",
    "- 환경을 상태는 (현재 택시가 있는 행, 현재 택시가 있는 열, 승객의 장소, 목적지)로 나타낼 수 있습니다.\n",
    "- 사실 상태를 4차원 벡터로 나타내지 않고, 가능한 상태마다 각각 0부터 499까지의 숫자를 붙여 상태로 사용합니다.\n",
    "- 가능한 모든 상태 $5 \\times 5 \\times 5 \\times 4 = 500$가지입니다. \n",
    "\n",
    "<br>\n",
    "\n",
    "**행동 공간**\n",
    "- 가능한 행동의 개수는 6개이며, 각각 다음을 나타냅니다.\n",
    "    - 0: move south\n",
    "    - 1: move north\n",
    "    - 2: move east\n",
    "    - 3: move west\n",
    "    - 4: pickup passenger\n",
    "    - 5: drop off passenger\n",
    "\n",
    "<br>\n",
    "\n",
    "**보상**\n",
    "- 목적지에서 승객을 하차 (drop off)시키면 +20의 보상을 받습니다.\n",
    "- 잘못된 위치에서 승하차 행동을 취하면 -10의 보상을 받습니다.\n",
    "- 다른 보상을 받지 않았다면, 매 스탭마다 -1을 보상을 받습니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "더 자세한 정보가 궁금하시면, 아래 링크를 참조해주세요.\n",
    "- https://www.gymlibrary.dev/environments/toy_text/taxi/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 만들기\n",
    "env = gym.make('Taxi-v3')\n",
    "env.seed(seed)\n",
    "\n",
    "print(\"가능한 상태의 개수: \", env.observation_space)\n",
    "print(\"가능한 행동의 개수: \", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경과 상호작용 (interaction)하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 초기화\n",
    "s, done, score, t = env.reset(), False, 0, 0\n",
    "\n",
    "# done이 True가 될 때까지 반복하여 환경과 상호작용\n",
    "while not done:\n",
    "    # 행동 선택 (이 코드에서는 임의의 행동을 취함)\n",
    "    a = np.random.randint(0, 6)\n",
    "    \n",
    "    # 행동 취하고 (다음 상태, 보상, 종료 여부, 기타 정보) 관찰\n",
    "    s_prime, r, done, info = env.step(a)\n",
    "    \n",
    "    # 누적 보상 계산하고, 다음 상태를 현재 상태로 바꿔준 후 타임스탭 종료\n",
    "    t += 1\n",
    "    score += r\n",
    "    s = s_prime\n",
    "    \n",
    "print(\"에피소드 점수: \", score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning 구현\n",
    "### 1. Q 테이블 정의하기\n",
    "- Q 테이블의 $s$행 $a$열에는 $Q^{*}(s, a)$의 추정치가 저장되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qtable = np.zeros((500, 6))  # Q 테이블 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 정책 구현하기\n",
    "- $\\epsilon$-greedy 정책은 주어진 상태 $s$에서\n",
    "    - $\\epsilon$의 확률로 임의의 행동을 취하고, \n",
    "    - $1 - \\epsilon$의 확률로 $Q(s, a)$ 값이 가장 큰 행동 $a$를 취합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon=0.1):\n",
    "    action = np.random.randint(6)\n",
    "    #########################################################\n",
    "    # 다음 빈칸을 채워주세요.\n",
    "    # 아래에서 `action` 변수를 수정해주지 않으면\n",
    "    # 위 코드에 의해서 임의의 행동을 취합니다.\n",
    "    # 힌트 1. np.random.rand(): 0과 1사이에서 임의의 수 뽑기\n",
    "    # 힌트 2. np.argmax(array): array에서 가장 큰 값의 인덱스를 반환\n",
    "    ########################################################\n",
    "    if (조건문):\n",
    "        action = \n",
    "\n",
    "    else:\n",
    "        action = \n",
    "    ########################################################\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Q-learning 구현 \n",
    "\n",
    "Q learning의 Q 테이블 업데이트식은 다음과 같습니다.\n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right).$$\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 학습에 이용될 매개변수들 정의하기\n",
    "아래 3개의 값을 바꿔가면서 멘토보다 더 좋은 에이전트를 만들어보세요. 물론 알고리즘 자체를 바꿔봐도 괜찮습니다. 예를 들어, \n",
    "- 몇몇 위치에서는 일부 행동이 벽에 막혀서 아무런 위치 변화를 만들어내지 못합니다. 이를 고려해볼 수도 있겠죠? \n",
    "- 아니면 고정된 $\\alpha$와 $\\epsilon$을 사용하지 않아도 괜찮습니다.\n",
    "- Q 테이블을 모두 꼭 0으로 초기화할 필요도 없습니다.\n",
    "- `np.argmax`는 동일한 값에 대해서 가장 빠른 인덱스를 불러옵니다. 동일한 최대값 중에 랜덤하게 뽑는 것도 생각해볼 수 있습니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "**멘토의 테스트 점수**\n",
    "- 평균 에피소드 보상:  -74.35\n",
    "- 평균 에피소드 길이:  86.95\n",
    "- 공정한 점수 비교를 위해 코드가 완성되면 `Kernel` 탭의 `Restart and Run All`을 통해 코드를 모두 실행해주세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "gamma = 0.6\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts = []\n",
    "scores = []\n",
    "Qtable = np.zeros((500, 6))\n",
    "for e in range(50000):\n",
    "\n",
    "    # 환경 초기화\n",
    "    s, done, score, t = env.reset(), False, 0, 0\n",
    "\n",
    "    # done이 True가 될 때까지 반복하여 환경과 상호작용\n",
    "    while not done:\n",
    "        # epsilon greedy 정책을 따라 행동을 선택\n",
    "        a = epsilon_greedy_policy(state=s, epsilon=epsilon)\n",
    "\n",
    "        # 행동 취하고 (다음 상태, 보상, 종료 여부, 기타 정보) 관찰\n",
    "        s_prime, r, done, info = env.step(a)\n",
    "\n",
    "        #########################################################\n",
    "        # 다음 빈칸을 채워주세요.\n",
    "        ########################################################\n",
    "        Qtable[s, a] = \n",
    "        ########################################################\n",
    "        \n",
    "        # 누적 보상 계산하고, 다음 상태를 현재 상태로 바꿔준 후 타임스탭 종료\n",
    "        t += 1\n",
    "        score += r\n",
    "        s = s_prime\n",
    "\n",
    "    if e % 1000 == 0:\n",
    "        ts.append(t)\n",
    "        scores.append(score)\n",
    "        print(f\"에피소드 {e:5}의 점수: {score:4}, 에피소드 길이: {t:3}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습 곡선 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 50000, 1000)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, scores, 'b-')\n",
    "plt.xlabel(\"Episode\", fontsize='x-large')\n",
    "plt.ylabel(\"Score\", fontsize='x-large')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, ts, 'b-')\n",
    "plt.xlabel(\"Episode\", fontsize='x-large')\n",
    "plt.ylabel(\"Episode length\", fontsize='x-large')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1000\n",
    "\n",
    "np.random.seed(seed)\n",
    "env = gym.make('Taxi-v3')\n",
    "env.seed(seed)\n",
    "ts = []\n",
    "frames = []\n",
    "scores = []\n",
    "for e in range(20):\n",
    "    # 환경 초기화\n",
    "    s, done, score, t = env.reset(), False, 0, 0\n",
    "\n",
    "    # done이 True가 될 때까지 반복하여 환경과 상호작용\n",
    "    while not done:\n",
    "        frames.append(env.render('ansi'))\n",
    "        \n",
    "        # epsilon greedy 정책을 따라 행동을 선택\n",
    "        a = epsilon_greedy_policy(state=s, epsilon=0.00)\n",
    "\n",
    "        # 행동 취하고 (다음 상태, 보상, 종료 여부, 기타 정보) 관찰\n",
    "        s_prime, r, done, info = env.step(a)\n",
    "\n",
    "        # 누적 보상 계산하고, 다음 상태를 현재 상태로 바꿔준 후 타임스탭 종료\n",
    "        t += 1\n",
    "        score += r\n",
    "        s = s_prime\n",
    "    \n",
    "    ts.append(t)\n",
    "    scores.append(score)\n",
    "\n",
    "print(\"평균 에피소드 보상: \", np.mean(scores))\n",
    "print(\"평균 에피소드 길이: \", np.mean(ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 테스트 때 나온 에이전트의 움직임입니다. 에이전트가 어떤 문제를 겪고 있나요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "for s in frames:\n",
    "    clear_output()\n",
    "    print(s)\n",
    "    time.sleep(0.05)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
